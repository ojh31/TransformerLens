{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "56161d6b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running as a Jupyter notebook - intended for development only!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_883/572068249.py:21: DeprecationWarning: `magic(...)` is deprecated since IPython 0.13 (warning added in 8.1), use run_line_magic(magic_name, parameter_s).\n",
      "  ipython.magic(\"load_ext autoreload\")\n",
      "/tmp/ipykernel_883/572068249.py:22: DeprecationWarning: `magic(...)` is deprecated since IPython 0.13 (warning added in 8.1), use run_line_magic(magic_name, parameter_s).\n",
      "  ipython.magic(\"autoreload 2\")\n"
     ]
    }
   ],
   "source": [
    "# Janky code to do different setup when run in a Colab notebook vs VSCode\n",
    "DEVELOPMENT_MODE = False\n",
    "try:\n",
    "    import google.colab\n",
    "    IN_COLAB = True\n",
    "    print(\"Running as a Colab notebook\")\n",
    "    %pip install git+https://github.com/neelnanda-io/TransformerLens.git``\n",
    "    %pip install circuitsvis\n",
    "    \n",
    "    # PySvelte is an unmaintained visualization library, use it as a backup if circuitsvis isn't working\n",
    "    # # Install another version of node that makes PySvelte work way faster\n",
    "    # !curl -fsSL https://deb.nodesource.com/setup_16.x | sudo -E bash -; sudo apt-get install -y nodejs\n",
    "    # %pip install git+https://github.com/neelnanda-io/PySvelte.git\n",
    "except:\n",
    "    IN_COLAB = False\n",
    "    print(\"Running as a Jupyter notebook - intended for development only!\")\n",
    "    from IPython import get_ipython\n",
    "\n",
    "    ipython = get_ipython()\n",
    "    # Code to automatically update the HookedTransformer code as its edited without restarting the kernel\n",
    "    ipython.magic(\"load_ext autoreload\")\n",
    "    ipython.magic(\"autoreload 2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "da9f5a40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import stuff\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import einops\n",
    "from fancy_einsum import einsum\n",
    "import tqdm.auto as tqdm\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "from pathlib import Path\n",
    "import plotly.express as px\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from torchtyping import TensorType as TT\n",
    "from typing import List, Union, Optional\n",
    "from jaxtyping import Float, Int\n",
    "from functools import partial\n",
    "import copy\n",
    "\n",
    "import itertools\n",
    "from transformers import AutoModelForCausalLM, AutoConfig, AutoTokenizer\n",
    "import dataclasses\n",
    "import datasets\n",
    "from IPython.display import HTML\n",
    "# import circuitsvis as cv\n",
    "\n",
    "import transformer_lens\n",
    "import transformer_lens.utils as utils\n",
    "from transformer_lens.hook_points import (\n",
    "    HookedRootModule,\n",
    "    HookPoint,\n",
    ")  # Hooking utilities\n",
    "from transformer_lens import HookedTransformer, HookedTransformerConfig, FactoredMatrix, ActivationCache\n",
    "\n",
    "torch.set_grad_enabled(False)\n",
    "\n",
    "def imshow(tensor, renderer=None, xaxis=\"\", yaxis=\"\", **kwargs):\n",
    "    px.imshow(utils.to_numpy(tensor), color_continuous_midpoint=0.0, color_continuous_scale=\"RdBu\", labels={\"x\":xaxis, \"y\":yaxis}, **kwargs).show(renderer)\n",
    "\n",
    "def line(tensor, renderer=None, xaxis=\"\", yaxis=\"\", **kwargs):\n",
    "    px.line(utils.to_numpy(tensor), labels={\"x\":xaxis, \"y\":yaxis}, **kwargs).show(renderer)\n",
    "\n",
    "def scatter(x, y, xaxis=\"\", yaxis=\"\", caxis=\"\", renderer=None, **kwargs):\n",
    "    x = utils.to_numpy(x)\n",
    "    y = utils.to_numpy(y)\n",
    "    px.scatter(y=y, x=x, labels={\"x\":xaxis, \"y\":yaxis, \"color\":caxis}, **kwargs).show(renderer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "030c22b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2LMHeadCustomModel(\n",
       "  (transformer): GPT2CustomModel(\n",
       "    (wte): Embedding(49280, 2048)\n",
       "    (wpe): Embedding(2048, 2048)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-23): 24 x GPT2CustomBlock(\n",
       "        (ln_1): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2MQAttention(\n",
       "          (q_attn): Conv1D()\n",
       "          (kv_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): FastGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=2048, out_features=49280, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load hf model\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "checkpoint = \"bigcode/santacoder\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "model = AutoModelForCausalLM.from_pretrained(checkpoint, trust_remote_code=True)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1f7ac1e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AutoModelForCausalLM.from_pretrained bigcode/santacoder torch.float32 {'trust_remote_code': True}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using pad_token, but it is not set yet.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT2LMHeadCustomModel(\n",
      "  (transformer): GPT2CustomModel(\n",
      "    (wte): Embedding(49280, 2048)\n",
      "    (wpe): Embedding(2048, 2048)\n",
      "    (drop): Dropout(p=0.1, inplace=False)\n",
      "    (h): ModuleList(\n",
      "      (0-23): 24 x GPT2CustomBlock(\n",
      "        (ln_1): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): GPT2MQAttention(\n",
      "          (q_attn): Conv1D()\n",
      "          (kv_attn): Conv1D()\n",
      "          (c_proj): Conv1D()\n",
      "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (ln_2): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): GPT2MLP(\n",
      "          (c_fc): Conv1D()\n",
      "          (c_proj): Conv1D()\n",
      "          (act): FastGELUActivation()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (ln_f): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=2048, out_features=49280, bias=False)\n",
      ")\n",
      "Loaded pretrained model bigcode/santacoder into HookedTransformer\n"
     ]
    }
   ],
   "source": [
    "# Disable folding norms and folding biases so that intermediate value\n",
    "# in between transformer blocks can be compared\n",
    "tl_model = HookedTransformer.from_pretrained(\n",
    "    checkpoint,fold_ln=False, fold_value_biases=False, center_writing_weights=False,\n",
    "    trust_remote_code=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1490c1cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg logits difference: -5.2463523303458715e-08\n",
      "max logits difference: 5.412101745605469e-05\n"
     ]
    }
   ],
   "source": [
    "text = '''\n",
    "TransformerLens lets you load in 50+ different open source language models,\n",
    "and exposes the internal activations of the model to you. You can cache\n",
    "any internal activation in the model, and add in functions to edit, remove\n",
    "or replace these activations as the model runs.\n",
    "'''\n",
    "input_ids = tokenizer(text, return_tensors='pt')['input_ids']\n",
    "gt_logits = model(input_ids)['logits'] # ground truth logits from hf\n",
    "my_logits = tl_model(input_ids)\n",
    "centered_gt_logits = gt_logits - gt_logits.mean(-1, keepdim=True)\n",
    "mean_diff = (my_logits.cpu() - centered_gt_logits).mean()\n",
    "print(\"avg logits difference:\", mean_diff.item())\n",
    "max_diff = (my_logits.cpu() - centered_gt_logits).abs().max()\n",
    "print(\"max logits difference:\", max_diff.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "48806097",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** Matching hf and T-Lens residual stream in between transformer blocks *****\n",
      "All layers match to atol=0.0001, rtol=1e-05\n"
     ]
    }
   ],
   "source": [
    "gt_cache = model(input_ids, output_hidden_states=True)['hidden_states']\n",
    "_, my_cache = tl_model.run_with_cache(input_ids)\n",
    "use_loose_bound = False\n",
    "pass_loose_bound = True\n",
    "atol = 1e-4\n",
    "rtol = 1e-5\n",
    "print(\"*\"*5, \"Matching hf and T-Lens residual stream in between transformer blocks\", \"*\"*5)\n",
    "for i in range(24):\n",
    "    try:\n",
    "        torch.testing.assert_close(my_cache['resid_pre',i], gt_cache[i].cuda(), atol=atol, rtol=rtol)\n",
    "    except:\n",
    "        max_diff = (my_cache['resid_pre',i] - gt_cache[i].cuda()).abs().max()\n",
    "        print(f\"layer {i} \\t not close, max difference: {max_diff}\")\n",
    "        use_loose_bound = True\n",
    "else: \n",
    "    print(f\"All layers match to atol={atol}, rtol={rtol}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "07bc8247",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T-Lens next token loss: 3.8878109455108643\n",
      "HF next token loss: 3.8877997398376465\n",
      "diff in loss (abs): 1.1205673217773438e-05\n"
     ]
    }
   ],
   "source": [
    "my_loss = tl_model(input_ids, return_type='loss')\n",
    "print(\"T-Lens next token loss:\", my_loss.item())\n",
    "gt_outputs = model(input_ids, labels=input_ids)\n",
    "gt_loss = gt_outputs.loss\n",
    "print(\"HF next token loss:\", gt_loss.item())\n",
    "print(\"diff in loss (abs):\", (gt_loss-my_loss).abs().item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74b0f24d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
